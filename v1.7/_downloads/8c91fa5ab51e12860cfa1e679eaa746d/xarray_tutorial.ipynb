{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# xarray with MetPy Tutorial\n\n[xarray](https://docs.xarray.dev/en/stable/) is a powerful Python package that provides\nN-dimensional labeled arrays and datasets following the Common Data Model. MetPy's suite of\nmeteorological calculations are designed to integrate with xarray DataArrays as one of its two\nprimary data models (the other being Pint Quantities). MetPy also provides DataArray and\nDataset *accessors* (collections of methods and properties attached to the ``.metpy`` property)\nfor coordinate/CRS and unit operations.\n\nFull information on MetPy's accessors is available in the :doc:`appropriate section of the\nreference guide </api/generated/metpy.xarray>`, otherwise, continue on in this\ntutorial for a demonstration of the three main components of MetPy's integration with xarray\n(coordinates/coordinate reference systems, units, and calculations), as well as instructive\nexamples for both CF-compliant and non-compliant datasets.\n\nFirst, some general imports...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport xarray as xr\n\n# Any import of metpy will activate the accessors\nimport metpy.calc as mpcalc\nfrom metpy.cbook import get_test_data\nfrom metpy.units import units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and opening some sample data to work with.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Open the netCDF file as a xarray Dataset\ndata = xr.open_dataset(get_test_data('irma_gfs_example.nc', False))\n\n# View a summary of the Dataset\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While xarray can handle a wide variety of n-dimensional data (essentially anything that can\nbe stored in a netCDF file), a common use case is working with gridded model output. Such\nmodel data can be obtained from a THREDDS Data Server using the [siphon package](https://unidata.github.io/siphon/), but here we've used an example subset of GFS data\nfrom Hurricane Irma (September 5th, 2017) included in MetPy's test suite. Generally,\na local file (or remote file via OPeNDAP) can be opened with ``xr.open_dataset(\"path\")``.\n\nGoing back to the above object, this ``Dataset`` consists of *dimensions* and their\nassociated *coordinates*, which in turn make up the axes along which the *data variables*\nare defined. The dataset also has a dictionary-like collection of *attributes*. What happens\nif we look at just a single data variable?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temperature = data['Temperature_isobaric']\ntemperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a ``DataArray``, which stores just a single data variable with its associated\ncoordinates and attributes. These individual ``DataArray``\\s are the kinds of objects that\nMetPy's calculations take as input (more on that in  `Calculations`_ section below).\n\nIf you are more interested in learning about xarray's terminology and data structures, see\nthe [terminology section](https://docs.xarray.dev/en/stable/terminology.html) of xarray's\ndocumentation.\n\n## Coordinates and Coordinate Reference Systems\n\nMetPy's first set of helpers comes with identifying *coordinate types*. In a given dataset,\ncoordinates can have a variety of different names and yet refer to the same type (such as\n\"isobaric1\" and \"isobaric3\" both referring to vertical isobaric coordinates). Following\nCF conventions, as well as using some fall-back regular expressions, MetPy can\nsystematically identify coordinates of the following types:\n\n- time\n- vertical\n- latitude\n- y\n- longitude\n- x\n\nWhen identifying a single coordinate, it is best to use the property directly associated\nwith that type\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temperature.metpy.time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When accessing multiple coordinate types simultaneously, you can use the ``.coordinates()``\nmethod to yield a generator for the respective coordinates\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x, y = temperature.metpy.coordinates('x', 'y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These coordinate type aliases can also be used in MetPy's wrapped ``.sel`` and ``.loc``\nfor indexing and selecting on ``DataArray``\\s. For example, to access 500 hPa heights at\n1800Z,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights = data['Geopotential_height_isobaric'].metpy.sel(\n    time='2017-09-05 18:00',\n    vertical=50000.\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Notice how we specified 50000 here without units...we'll go over a better alternative in\nthe next section on units.)\n\nOne point of warning: xarray's selection and indexing only works if these coordinates are\n*dimension coordinates*, meaning that they are 1D and share the name of their associated\ndimension. In practice, this means that you can't index a dataset that has 2D latitude and\nlongitude coordinates by latitudes and longitudes, instead, you must index by the 1D y and x\ndimension coordinates. (What if these coordinates are missing, you may ask? See the final\nsubsection on ``.assign_y_x`` for more details.)\n\nBeyond just the coordinates themselves, a common need for both calculations with and plots\nof geospatial data is knowing the coordinate reference system (CRS) on which the horizontal\nspatial coordinates are defined. MetPy follows the [CF Conventions](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.html#grid-mappings-and-projections)\nfor its CRS definitions, which it then caches on the ``metpy_crs`` coordinate in order for\nit to persist through calculations and other array operations. There are two ways to do so\nin MetPy:\n\nFirst, if your dataset is already conforming to the CF Conventions, it will have a grid\nmapping variable that is associated with the other data variables by the ``grid_mapping``\nattribute. This is automatically parsed via the ``.parse_cf()`` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Parse full dataset\ndata_parsed = data.metpy.parse_cf()\n\n# Parse subset of dataset\ndata_subset = data.metpy.parse_cf([\n    'u-component_of_wind_isobaric',\n    'v-component_of_wind_isobaric',\n    'Vertical_velocity_pressure_isobaric'\n])\n\n# Parse single variable\nrelative_humidity = data.metpy.parse_cf('Relative_humidity_isobaric')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your dataset doesn't have a CF-conforming grid mapping variable, you can manually specify\nthe CRS using the ``.assign_crs()`` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temperature = data['Temperature_isobaric'].metpy.assign_crs(\n    grid_mapping_name='latitude_longitude',\n    earth_radius=6371229.0\n)\n\ntemperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the newly added ``metpy_crs`` non-dimension coordinate. Now how can we use this in\npractice? For individual ``DataArrays``\\s, we can access the cartopy and pyproj objects\ncorresponding to this CRS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cartopy CRS, useful for plotting\nrelative_humidity.metpy.cartopy_crs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# pyproj CRS, useful for projection transformations and forward/backward azimuth and great\n# circle calculations\ntemperature.metpy.pyproj_crs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, there are times when a certain horizontal coordinate type is missing from your\ndataset, and you need the other, that is, you have latitude/longitude and need y/x, or visa\nversa. This is where the ``.assign_y_x`` and ``.assign_latitude_longitude`` methods come in\nhandy. Our current GFS sample won't work to demonstrate this (since, on its\nlatitude-longitude grid, y is latitude and x is longitude), so for more information, take\na look at the `Non-Compliant Dataset Example`_ below, or view the accessor documentation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Units\n\nSince unit-aware calculations are a major part of the MetPy library, unit support is a major\npart of MetPy's xarray integration!\n\nOne very important point of consideration is that xarray data variables (in both\n``Dataset``\\s and ``DataArray``\\s) can store both unit-aware and unit-naive array types.\nUnit-naive array types will be used by default in xarray, so we need to convert to a\nunit-aware type if we want to use xarray operations while preserving unit correctness. MetPy\nprovides the ``.quantify()`` method for this (named since we are turning the data stored\ninside the xarray object into a Pint ``Quantity`` object)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights = heights.metpy.quantify()\nheights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the units are now represented in the data itself, rather than as a text\nattribute. Now, even if we perform some kind of xarray operation (such as taking the zonal\nmean), the units are preserved\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights_mean = heights.mean('longitude')\nheights_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, this \"quantification\" is not without its consequences. By default, xarray loads its\ndata lazily to conserve memory usage. Unless your data is chunked into a Dask array (using\nthe ``chunks`` argument), this ``.quantify()`` method will load data into memory, which\ncould slow your script or even cause your process to run out of memory. And so, we recommend\nsubsetting your data before quantifying it.\n\nAlso, these Pint ``Quantity`` data objects are not properly handled by xarray when writing\nto disk. And so, if you want to safely export your data, you will need to undo the\nquantification with the ``.dequantify()`` method, which converts your data back to a\nunit-naive array with the unit as a text attribute\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights_mean_str_units = heights_mean.metpy.dequantify()\nheights_mean_str_units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other useful unit integration features include:\n\nUnit-based selection/indexing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights_at_45_north = data['Geopotential_height_isobaric'].metpy.sel(\n    latitude=45 * units.degrees_north,\n    vertical=300 * units.hPa\n)\nheights_at_45_north"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unit conversion:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temperature_degc = temperature[0].metpy.convert_units('degC')\ntemperature_degc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To base unit conversion:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temperature_degk = temperature_degc.metpy.convert_to_base_units()\ntemperature_degk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unit conversion for coordinates:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights_on_hpa_levels = heights.metpy.convert_coordinate_units('isobaric3', 'hPa')\nheights_on_hpa_levels['isobaric3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing just the underlying unit array:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights_unit_array = heights.metpy.unit_array\nheights_unit_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing just the underlying units:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "height_units = heights.metpy.units\nheight_units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculations\n\nMetPy's xarray integration extends to its calculation suite as well. Most grid-capable\ncalculations (such as thermodynamics, kinematics, and smoothers) fully support xarray\n``DataArray``\\s by accepting them as inputs, returning them as outputs, and automatically\nusing the attached coordinate data/metadata to determine grid arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "heights = data_parsed.metpy.parse_cf('Geopotential_height_isobaric').metpy.sel(\n    time='2017-09-05 18:00',\n    vertical=500 * units.hPa\n)\nu_g, v_g = mpcalc.geostrophic_wind(heights)\nu_g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For profile-based calculations (and most remaining calculations in the ``metpy.calc``\nmodule), xarray ``DataArray``\\s are accepted as inputs, but the outputs remain Pint\nQuantities (typically scalars). Note that MetPy's profile calculations (such as CAPE and\nCIN) require the sounding to be ordered from highest to lowest pressure. As seen earlier\nin this tutorial, this data is ordered the other way, so we need to reverse the inputs\nto ``mpcalc.surface_based_cape_cin``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_at_point = data.metpy.sel(\n    time1='2017-09-05 12:00',\n    latitude=30 * units.degrees_north,\n    longitude=260 * units.degrees_east\n)\ndewpoint = mpcalc.dewpoint_from_relative_humidity(\n    data_at_point['Temperature_isobaric'],\n    data_at_point['Relative_humidity_isobaric']\n)\ncape, cin = mpcalc.surface_based_cape_cin(\n    data_at_point['isobaric3'][::-1],\n    data_at_point['Temperature_isobaric'][::-1],\n    dewpoint[::-1]\n)\ncape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few remaining portions of MetPy's calculations (mainly the interpolation module and a few\nother functions) do not fully support xarray, and so, use of ``.values`` may be needed to\nconvert to a bare NumPy array. For full information on xarray support for your function of\ninterest, see the :doc:`/api/index`.\n\n## CF-Compliant Dataset Example\n\nThe GFS sample used throughout this tutorial so far has been an example of a CF-compliant\ndataset. These kinds of datasets are easiest to work with it MetPy, since most of the\n\"xarray magic\" uses CF metadata. For this kind of dataset, a typical workflow looks like the\nfollowing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load data, parse it for a CF grid mapping, and promote lat/lon data variables to coordinates\ndata = xr.open_dataset(\n    get_test_data('narr_example.nc', False)\n).metpy.parse_cf().set_coords(['lat', 'lon'])\n\n# Subset to only the data you need to save on memory usage\nsubset = data.metpy.sel(isobaric=500 * units.hPa)\n\n# Quantify if you plan on performing xarray operations that need to maintain unit correctness\nsubset = subset.metpy.quantify()\n\n# Perform calculations\nheights = mpcalc.smooth_gaussian(subset['Geopotential_height'], 5)\nsubset['u_geo'], subset['v_geo'] = mpcalc.geostrophic_wind(heights)\n\n# Plot\nheights.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save output\nsubset.metpy.dequantify().drop_vars('metpy_crs').to_netcdf('500hPa_analysis.nc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Compliant Dataset Example\n\nWhen CF metadata (such as grid mapping, coordinate attributes, etc.) are missing, a bit more\nwork is required to manually supply the required information, for example,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nonstandard = xr.Dataset({\n    'temperature': (('y', 'x'), np.arange(0, 9).reshape(3, 3) * units.degC),\n    'y': ('y', np.arange(0, 3) * 1e5, {'units': 'km'}),\n    'x': ('x', np.arange(0, 3) * 1e5, {'units': 'km'})\n})\n\n# Add both CRS and then lat/lon coords using chained methods\ndata = nonstandard.metpy.assign_crs(\n    grid_mapping_name='lambert_conformal_conic',\n    latitude_of_projection_origin=38.5,\n    longitude_of_central_meridian=262.5,\n    standard_parallel=38.5,\n    earth_radius=6371229.0\n).metpy.assign_latitude_longitude()\n\n# Preview the changes\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the CRS and additional coordinates are assigned, you can generally proceed as you would\nfor a CF-compliant dataset.\n\n## What Could Go Wrong?\n\nDepending on your dataset and what you are trying to do, you might run into problems with\nxarray and MetPy. Below are examples of some of the most common issues\n\n- Multiple coordinate conflict\n- An axis not being available\n- An axis not being interpretable\n- ``UndefinedUnitError``\n\n**Coordinate Conflict**\n\nCode:\n\n::\n\n    x = data['Temperature'].metpy.x\n\nError Message:\n\n::\n\n    /home/user/env/MetPy/metpy/xarray.py:305: UserWarning: More than\n    one x coordinate present for variable \"Temperature\".\n\nFix:\n\nManually assign the coordinates using the ``assign_coordinates()`` method on your DataArray,\nor by specifying the ``coordinates`` argument to the ``parse_cf()`` method on your Dataset,\nto map the ``time``, ``vertical``, ``y``, ``latitude``, ``x``, and ``longitude`` axes (as\napplicable to your data) to the corresponding coordinates.\n\n::\n\n    data['Temperature'].assign_coordinates({'time': 'time', 'vertical': 'isobaric',\n                                            'y': 'y', 'x': 'x'})\n    x = data['Temperature'].metpy.x\n\nor\n\n::\n\n    temperature = data.metpy.parse_cf('Temperature',\n                                      coordinates={'time': 'time', 'vertical': 'isobaric',\n                                                   'y': 'y', 'x': 'x'})\n    x = temperature.metpy.x\n\n**Axis Unavailable**\n\nCode:\n\n::\n\n    data['Temperature'].metpy.vertical\n\nError Message:\n\n::\n\n    AttributeError: vertical attribute is not available.\n\nThis means that your data variable does not have the coordinate that was requested, at\nleast as far as the parser can recognize. Verify that you are requesting a\ncoordinate that your data actually has, and if it still is not available,\nyou will need to manually specify the coordinates as discussed above.\n\n**Axis Not Interpretable**\n\nCode:\n\n::\n\n    x, y, ensemble = data['Temperature'].metpy.coordinates('x', 'y', 'ensemble')\n\nError Message:\n\n::\n\n    AttributeError: 'ensemble' is not an interpretable axis\n\nThis means that you are requesting a coordinate that MetPy is (currently) unable to parse.\nWhile this means it cannot be recognized automatically, you can still obtain your desired\ncoordinate directly by accessing it by name. If you have a need for systematic\nidentification of a new coordinate type, we welcome pull requests for such new functionality\non GitHub!\n\n**Undefined Unit Error**\n\nIf the units attribute on your xarray data is not recognizable by Pint, you will likely\nreceive an ``UndefinedUnitError``. In this case, you will likely have to update the units\nattribute to one that can be parsed properly by Pint. It is our aim to have all valid\nCF/UDUNITS unit strings be parseable, but this work is ongoing. If many variables in your\ndataset are not parseable, the ``.update_attribute`` method on the MetPy accessor may come\nin handy.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}